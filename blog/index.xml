<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tim Cosemans</title>
<link>https://timcosemans.be/blog/</link>
<atom:link href="https://timcosemans.be/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Tim is a data scientist working at the intersection of AI, business and education.</description>
<generator>quarto-1.8.27</generator>
<lastBuildDate>Sun, 24 Aug 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>AI-nuchterheid</title>
  <dc:creator>Tim Cosemans</dc:creator>
  <link>https://timcosemans.be/blog/ai_nuchterheid/</link>
  <description><![CDATA[ 




<p>Knipper twee keer en je geraakt onherroepelijk achterop in het oerwoud aan nieuwe AI-modellen en -toepassingen. De techgiganten vliegen elkaar de afgelopen maanden onophoudelijk in de haren en dat levert elke week minstens één nieuwe vakterm op die we ons eigen moeten maken. Met de nadruk op moeten, want de beloftes zijn hoog. En je wil de AI-trein toch niet missen?</p>
<section id="waarom-doen-we-dit-eigenlijk" class="level1">
<h1>Waarom doen we dit eigenlijk?</h1>
<p>Je kan het ze niet kwalijk nemen daar in <em>Silicon Valley</em>. De geconcentreerde smeltkroes van ingenieurs en wetenschappers staat te springen voor elke nieuwigheid. Ze zijn ‘innovators’, altijd op zoek naar het nieuwste hoogtechnologische gat in de markt. Hoe groot het gat is, maakt eigenlijk niet uit. Zolang het naar gevuld kan worden. Het is nog maar af te wachten of de zoveelste iteratie van de slimme brillen van Meta, of het nu met AR, VR of AI is, het dit keer wel verder zullen brengen dan de YouTubekanalen van de <em>techfluencers</em>. Wij kunnen niet anders dan ernaar kijken als een kip naar het onweer. Gefascineerd, onder de indruk, en zonder besef van wat er nog komen zal. Maar de Meta’s en de Google’s van deze wereld hebben grotere doelen. Ze mikken hoger. Wat we nu zien zijn slechts bijproducten in de race naar waar het allemaal echt om draait: <em>artificial general intelligence</em>. Of nog beter <em>artificial superintelligence</em>. Het benaderen of zelfs voorbijstreven van de menselijke intelligentie door een machine.</p>
</section>
<section id="kan-dat-eigenlijk-wel" class="level1">
<h1>Kan dat eigenlijk wel?</h1>
<p>Terwijl de psychologie na jaren onderzoek nog steeds geen allesomvattende definitie kan geven van menselijke intelligentie, hebben de computerwetenschappen er alvast een antwoord op: <em>benchmarks</em>. Oftewel vooraf gedefinieerde taken, van gestandaardiseerde examens tot vertalingen, met een duidelijk juist antwoord. Nu ik weet dat onze volledige intellectuele capaciteiten vervat kunnen worden in staafdiagrammen, lijkt psycholoog me opeens geen al te moeilijk beroep meer. Microsoft concludeert daarom in een recent onderzoek dat heel wat banen op de schop kunnen. Wat handig dat zij al even alles op alles zetten om hun AI-assistent, <em>Copilot</em>, aan de man te krijgen. Het lijstje van die beroepen? Dat loopt uiteen van schrijvers tot wiskundigen. Iedereen weet toch dat de beste romans en het meest baanbrekende onderzoek gebeurt door mensen die enkel reeds bestaand werken samenvatten, toch?</p>
</section>
<section id="wat-brengt-de-toekomst-dan" class="level1">
<h1>Wat brengt de toekomst dan?</h1>
<p>Het eerste techbedrijf dat de eindmeet in de race om <em>artificial general intelligence</em> of zelfs <em>artificial superintelligence</em> weet te halen kan aandeelhouders een gouden toekomst verzekeren. Hoe die eindmeet er precies uitziet, daar blijven de CEO’s van de eerdergenoemde techbedrijven vaag over. Maar in de filmpjes en interviews die aan de lopende band opduiken zie je hun Amerikaanse ogen fonkelen. Van verwachting of angst, dat weten we helaas nog niet. Bij Sam Altman, de CEO van OpenAI, lijkt het er alvast op dat het dat laatste is. Hij vergelijkt de komst van het, nota bene door zijn eigen bedrijf ontwikkelde, GPT-5 met die van de atoombom. Mark Zuckerberg ziet er dan alweer geen graat in en droomt luidop van een poeslieve superintelligentie, dat gedwee onze saaie taakjes zal uitvoeren. (Iemand interesse in een AI Agent die wel aan je belastingaangifte uitkan?) En dat allemaal berustend op zijn recent samengestelde miljoenenteam van knappe koppen. Wat nog moet blijken is of het amalgaam van de knapste koppen ook het knapste team vormt. Mijn oud professor groepsdynamica zou dat alvast tegenspreken. Wat Mark ook gemakshalve vergeet te vermelden is dat zijn bedrijf de Europese AI Act links laat liggen. Of de technologie poeslief wordt weten we nog niet, maar ethisch ontwikkeld zal hij alvast niet zijn. Laten we niet doen alsof wetgeving de heilige graal is. Zelfs de meer meegaande bedrijven, zoals Google, zijn niet onbekend met het geweer van schouder te wisselen als hen dat een voordeel kan opleveren. Want de politieke macht kan zonder veel problemen de geldtoevoer afknippen, dus laten we vooral zorgen dat we bij hen op een goed blaadje staan. Ook al betekent dat dat we ideologisch twijfelachtige regimes moeten steunen. Zijn dat de bedrijven waar we dagelijks onze Europese bedrijfsgeheimen naartoe pompen omdat we te lui zijn om te zoeken naar de omzet van het afgelopen kwartaal in het jaarlijks financieel rapport?</p>
</section>
<section id="en-nu" class="level1">
<h1>En nu?</h1>
<p>Begrijp me niet verkeerd. Ik kan de komst van de open-source AI-modellen en de tools die ervoor worden ontwikkeld alleen maar toejuichen. De technologie belooft ons leven alleen maar makkelijker te maken. Ze zou zomaar eens een chatbot kunnen opleveren die de volgende generatie aan leerlingen een antwoord kan geven op de vraag waarom de Westerse landen hun ogen zo lang hebben gesloten voor de oorlog in Gaza. Of beter nog, ze zou je een antwoord kunnen geven op de vraag hoeveel vrije dagen je nog op overschot hebt zonder daarbij Mark op de hoogte te brengen van je vakantieplannen.</p>


</section>

 ]]></description>
  <category>data ethics</category>
  <category>data science</category>
  <guid>https://timcosemans.be/blog/ai_nuchterheid/</guid>
  <pubDate>Sun, 24 Aug 2025 00:00:00 GMT</pubDate>
  <media:content url="https://timcosemans.be/blog/ai_nuchterheid/header.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A note on panel data methods</title>
  <dc:creator>Tim Cosemans</dc:creator>
  <link>https://timcosemans.be/blog/panel_data_note/</link>
  <description><![CDATA[ 




<section id="defining-panel-data" class="level1 page-columns page-full">
<h1>Defining Panel Data</h1>
<p>Panel, or longitudinal data has become widely available to empirical researchers across economics. Many countries conduct national periodic surveys, such as the British Household Panel Survey or the National Longitudinal Survey in the United States. In addition, commercial organizations offer a wide array of longitudinal datasets for research, such as Nielsen’s Consumer Panel. The units of analysis, individuals, firms etc., are not only observed on different variables (like in cross-sectional data), but also over time. Key to this type of data is the fact that the observations, i.e., the individual data points, are no longer independent since they belong to the same unit of analysis. The most outspoken benefit of this type of data lies in this within-unit variance, that brings extra information in addition to the traditional between-unit variance observed in cross-sectional data <span class="citation" data-cites="capitaine">(Capitaine, Genuer, and Thiébaut 2021)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-capitaine" class="csl-entry">
Capitaine, Louis, Robin Genuer, and Rodolphe Thiébaut. 2021. <span>‘Random Forests for High-Dimensional Longitudinal Data’</span>. <em>Statistical Methods in Medical Research</em> 30 (1): 166–84.
</div></div><p>Panel or longitudinal data is therefore a special case of clustered data, where data points belong to a cluster that causes a correlation between them. A different type of clustered data is hierarchical data where data points are connected not because they belong to the same unit of analysis, but because the unit of analysis belongs to some overarching group. For example, test scores of students have a hierarchical dimension since the unit of analysis, the student, belongs to a group, a class. All grades from students of the same class can be expected to show some correlation due to characteristics of the class, such as the teacher, the class size etc.</p>
</section>
<section id="analyzing-panel-data" class="level1 page-columns page-full">
<h1>Analyzing Panel Data</h1>
<p>Both econometrics and statistics have a large array of methods to analyze panel data, but the terminology used to denote them can often be conflicting and confusing <span class="citation" data-cites="gelman">(Gelman 2005)</span>. In general, we can define a longitudinal model with individual specific intercepts as follows:</p>
<div class="no-row-height column-margin column-container"><div id="ref-gelman" class="csl-entry">
Gelman, Andrew. 2005. <span>‘Why i Don’t Use the Term <span>“Fixed and Random Effects”</span> | Statistical Modeling, Causal Inference, and Social Science’</span>. <a href="https://statmodeling.stat.columbia.edu/2005/01/25/why_i_dont_use/">https://statmodeling.stat.columbia.edu/2005/01/25/why_i_dont_use/</a>.
</div></div><p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20f(X_%7Bit%7D)%20+%20c_i%20+%20e_%7Bit%7D%0A"></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(X_%7Bit%7D)"> is an unknown function<br>
</li>
<li><img src="https://latex.codecogs.com/png.latex?Y_%7Bit%7D"> represents the outcome<br>
</li>
<li><img src="https://latex.codecogs.com/png.latex?e_%7Bit%7D"> is a standard normally distributed error<br>
</li>
<li><img src="https://latex.codecogs.com/png.latex?c_i"> is an unobserved, time-constant factor specific to individual <em>i</em></li>
</ul>
<p>In econometrics, this model is called the unobserved effects model <span class="citation" data-cites="wooldridge">(Wooldridge 2016)</span>. There, the discussion usually centers around whether <img src="https://latex.codecogs.com/png.latex?c_i"> must be treated as a random variable (i.e., a random effect) or a parameter to be estimated (i.e., a fixed effect).</p>
<div class="no-row-height column-margin column-container"></div><p>Over time:</p>
<ul>
<li>“Random effects” has become synonymous with zero correlation between <img src="https://latex.codecogs.com/png.latex?c_i"> and <img src="https://latex.codecogs.com/png.latex?X_i"><br>
</li>
<li>“Fixed effects” implies correlation between <img src="https://latex.codecogs.com/png.latex?c_i"> and <img src="https://latex.codecogs.com/png.latex?X_i"></li>
</ul>
<p>In statistics, on the other hand, a fixed effect is a common term used to refer to any parameter in a regression model that is a population average to be estimated, usually denoted by <img src="https://latex.codecogs.com/png.latex?%5Cbeta">.</p>
<p>Estimation under the assumption of non-zero correlation in econometrics (“fixed effects estimation”) is more realistic, but requires the removal of the influence of <img src="https://latex.codecogs.com/png.latex?c_i">. This can be done using the “within” transformation of the equation to be estimated, which is equivalent to estimating with OLS</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20-%20%C8%B2_i%20=%20f(X_%7Bit%7D%20-%20X%CC%84_i)%20+%20e_%7Bit%7D%20-%20%C4%93_i%0A"></p>
<p>While this approach successfully removes the influence of <img src="https://latex.codecogs.com/png.latex?c_i">, it also removes all time-constant variables. The main downside of this approach is that other time-constant variables cannot be included. Estimation under the assumption of no correlation (“random effects estimation”), does allow for the inclusion of time-constant variables, but is often less realistic. This involves estimating</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20f(X_%7Bit%7D)%20+%20v_%7Bit%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?v_%7Bit%7D%20=%20c_i%20+%20e_%7Bit%7D">. Econometricians account for the serial correlation introduced in the composite error term by using generalised least squares. Similar approaches exist in statistics to model the influence of individual-specific factors. They all fall under the “generalised linear mixed model” (GLMM). The general linear mixed model can be written as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20f(X_%7Bit%7D)%20+%20W_%7Bit%7D%20*%20c_i%20+%20e_%7Bit%7D%0A"></p>
<p>Where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D"> are covariates with fixed effects<br>
</li>
<li><img src="https://latex.codecogs.com/png.latex?W_%7Bit%7D"> are covariates with random effects</li>
</ul>
<p>These last two equations are equivalent if we only include a random intercept (<img src="https://latex.codecogs.com/png.latex?W_%7Bit%7D%20=%201">). What the econometrician calls unobserved effects, the statistician calls random intercepts.</p>
<p>Caution is needed, however. While the econometrician’s random effects model is equivalent to the statistician’s random intercept model, estimating both (e.g., using the plm and lmer packages in R) can yield different results as mixed models are usually estimated using (restricted or unrestricted) maximum likelihood and not generalised least squares.</p>
<p><em>“The econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on the machine, the expressions for the estimators are usually rather simple. ML estimation of longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence criteria.”</em> <span class="citation" data-cites="croissant">(Croissant and Millo 2008)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-croissant" class="csl-entry">
Croissant, Yves, and Giovanni Millo. 2008. <span>‘Panel Data Econometrics in r: The Plm Package’</span>. <em>Journal of Statistical Software</em> 27: 1–43.
</div></div><p>The GLMM does add the opportunity to also add random, individual specific, slopes for covariates in addition to their general, fixed effect. This can be done by including a variable in both <img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D"> and <img src="https://latex.codecogs.com/png.latex?W_%7Bit%7D">. These random effects represent random variation in the model that can be attributed to a group to which observations belong. It is with these types of models that opportunities arise to model not only longitudinal but also hierarchical effects simultaneously.</p>
<p>In statistics, the econometrician’s worry about violation of the “random effects” assumption still holds when working with mixed models and observational data. There might still be correlation between the error term and the explanatory variables due to other time-constant unobserved effects. Experimental data does not suffer from this type of endogeneity.</p>
<p>Specifically for the case of the random intercept, which is the most prevalent in econometrics, we can employ a “correlated random effects approach”, which explicitly models the correlation between <img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D"> and <img src="https://latex.codecogs.com/png.latex?c_i">. This approach assumes</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ac_i%20=%20%5Cpsi%20+%20X%CC%84_i%20*%20%5Cphi%20+%20a_i%0A"></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%CC%84_i"> is the vector of time-averages of all variables</li>
<li><img src="https://latex.codecogs.com/png.latex?E(a_i%7CX_i)%20=%200"></li>
</ul>
<p>So that we can estimate:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20f(X_%7Bit%7D)%20+%20%5Cpsi%20+%20Z_i%20*%20%5Cdelta%20+%20X%CC%84_i%20*%20%5Cphi%20+%20v_%7Bit%7D%0A"></p>
<p>Econometricians again use GLS to deal with serial correlation in the composite error <img src="https://latex.codecogs.com/png.latex?v_%7Bit%7D">,all while allowing other time-constant factors and accounting for unobserved time-constant effects that are correlated with <img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D"> <span class="citation" data-cites="wooldridge">(Wooldridge 2016)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wooldridge" class="csl-entry">
Wooldridge, Jeffrey. 2016. <em>Introductory Econometrics</em>. Cengage AU.
</div></div></section>
<section id="final-note" class="level1">
<h1>Final Note</h1>
<p>In addition to the methods described above, econometrics has a different set of methods for analyzing ‘dynamic’ panel data methods, that include lags of variables. I consider these to be outside of the scope of the current discussion.</p>



</section>

 ]]></description>
  <category>data science</category>
  <category>econometrics</category>
  <guid>https://timcosemans.be/blog/panel_data_note/</guid>
  <pubDate>Sun, 06 Apr 2025 00:00:00 GMT</pubDate>
  <media:content url="https://timcosemans.be/blog/panel_data_note/header.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The struggles of a data marketer</title>
  <dc:creator>Tim Cosemans</dc:creator>
  <link>https://timcosemans.be/blog/struggles_data_marketeer/</link>
  <description><![CDATA[ 




<p>Marketing is traditionally associated with advertising but is so much more than deciding on which ads to present to your customers. As a marketer, you’re expected to help shape your company’s value proposition, communicate it to your customers and manage their acquisition, development, and churn. Yet this process is not without its problems: Your customers might leave your company; you might face high shopping cart abandonment or have to deal with low conversion rates. As a marketer you will most likely turn to your data to see what is going on. And evidence suggests you should: Reports from McKinsey show that data-driven sales can increase EBITDA by 15 to 25 percent <span class="citation" data-cites="boringer">(Böringer et al. 2022)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-boringer" class="csl-entry">
Böringer, J., A. Dierks, I. Huber, and D. Spillecke. 2022. <span>‘Insights to Impact: Creating and Sustaining Data-Driven Commercial Growth.’</span> McKinsey &amp; Company. <a href="ttps://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/insights-to-impact-creating-and-sustaining-data-driven-commercial-growth">ttps://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/insights-to-impact-creating-and-sustaining-data-driven-commercial-growth</a>.
</div></div><p>Yet your data analysis process can take too long. You have to start from scratch collecting data each time or your analysis methods cannot seem to uncover what is truly happening among your customers. A recent study by the Chief Marketing Officer Council (2022) in partnership with GfK, one of the world’s largest marketing research organisations, shows you are not alone:</p>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>“80% Of marketers say data, analytics and insights are very important to winning and retaining customers. Yet nearly two-thirds of those marketers are only moderately confident (or worse) in their data, analytics, and insights systems.” <span class="citation" data-cites="cmocouncil">(<span>‘The High-Velocity Data Marketer’</span> 2022)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<p>Data marketing relies on access to all relevant data in order to find answers to business questions. Data access comes with its own barriers however (Figure 1). Often marketeers find themselves unable to integrate all data due to insufficient tools or technology. The files coming from your company database might not be compatible with your preferred analytics software or they might be too large to handle on your personal computer. Moving all your data to a cloud provider, such as Microsoft, Google or Amazon can be a great help in such cases. In choosing such a provider, it is important to consider what needs you need fulfilled. Want to make nice looking dashboards in PowerBI? Microsoft Azure might be the way to go. Want to easily access your data from Google Analytics? Think about Google Cloud Platform as your provider.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://timcosemans.be/blog/struggles_data_marketeer/data_barriers.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Main barriers to data access <span class="citation" data-cites="cmocouncil">(<span>‘The High-Velocity Data Marketer’</span> 2022)</span></figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>Making such a decision is however not easy, and your company’s situation might have other requirements. In addition, lack of proper data management processes and scattered data control make it even harder for marketers to gather all their data in one place. Before you make cloud decisions, it is therefore always a good idea to conduct an analytical maturity assessment. What data does your company have? Where is it situated? How do you plan on using it? And what should the results look like?</p>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>“81% Of marketers say hiring more talent with AI skills is the most critical need to developing their AI capabilities. Yet hiring more talent is also their biggest AI-related challenge.” <span class="citation" data-cites="cmocouncil">(<span>‘The High-Velocity Data Marketer’</span> 2022)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<p>The customer journey has become more digitalised throughout the years: Customers do their research online before even meeting a salesperson. These buyer intent signals are critical in understanding the customer’s decision process and its critical moments.</p>
<p>Buyer intent signals can come from multiple sources, including your website, social media, or internal CRM systems. True competitive advantage comes from generating real-time insights, identifying sudden shifts in these signals, and swiftly adapting and acting on them. This can mean searching for and addressing bottle necks in your customer’s journey or adapting your messages to your customers. At Algorhythm we addressed low engagement among customers of a large pharmaceutical player by building models that provide customer level recommendations on which channels to use, frequency to interact, and content to communicate, for example.</p>
<p>As the noise across all of these channels grows louder, data marketers have to work even harder to find, extract and analyse relevant signals and produce actionable insights all while keeping the limited time span of the customer’s interactions with the company in mind. Data marketers therefore need the capabilities to detect opportunities and decide on the next best action while the customer is still engaged with the organisation. The banking sector, for example, often struggles with preventing customer churn. For our client, a large Belgian bank, we built a model to predict not only the propensity of churn but also the best possible offer to address each client with.</p>
<p>AI has disrupted the standard toolkit of the data marketeer but also given us new tools to uncover even the most well-hidden patterns. The same study by GfK (Figure 2) shows that capabilities like predictive and prescriptive analytics or sentiment analysis are however still out of most marketers’ reach. The abundance of technologies and methods might be overwhelming, but all serve their individual purposes. It is therefore important to ask yourself the right questions before getting started with any of them. Do you want to know which customers will churn within a given time frame? Then you might want to focus on predictive analytics. But do you want to know what type of campaigns you should run in order to attract new customers? Then prescriptive analytics is probably more suited for your purposes. Even scraping online sources to reveal customer sentiment about your newly launched products or ad campaigns before they are reflected in the company’s financials is no longer out of reach with AI.</p>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>“The challenge is choosing the right solution from a sea of options.” <span class="citation" data-cites="cmocouncil">(<span>‘The High-Velocity Data Marketer’</span> 2022)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<p>The struggles of a data marketer are real. And as data becomes more ubiquitous and AI more pervasive, they are not likely to become any less prevalent.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://timcosemans.be/blog/struggles_data_marketeer/data_challenges.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Data capabilities still out of reach for marketers <span class="citation" data-cites="cmocouncil">(<span>‘The High-Velocity Data Marketer’</span> 2022)</span></figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-cmocouncil" class="csl-entry">
<span>‘The High-Velocity Data Marketer’</span>. 2022. CMO Council. <a href="https://www.cmocouncil.org/thought-leadership/reports/the-high-velocity-data-marketer">https://www.cmocouncil.org/thought-leadership/reports/the-high-velocity-data-marketer</a>.
</div></div></figure>
</div>




 ]]></description>
  <category>data strategy</category>
  <category>data science</category>
  <category>marketing analytics</category>
  <guid>https://timcosemans.be/blog/struggles_data_marketeer/</guid>
  <pubDate>Tue, 07 Feb 2023 00:00:00 GMT</pubDate>
  <media:content url="https://timcosemans.be/blog/struggles_data_marketeer/header.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Explainable convolutional neural networks for time series classification</title>
  <dc:creator>Tim Cosemans</dc:creator>
  <link>https://timcosemans.be/blog/explainable_cnns/</link>
  <description><![CDATA[ 




<p>Data science is a rapidly evolving field. Daily, academics at universities work on the next big algorithm that will help make our lives easier. Large corporations, such as Microsoft and Amazon, have even made in-house research part of their competitive advantage by founding their own institutes covering research in areas ranging from economics and health to human-computer interaction and machine learning. Meta, previously Facebook, <a href="https://www.reuters.com/technology/facebook-owner-meta-opens-access-ai-large-language-model-2022-05-03/">recently opened up access to a 175-billion-parameter language model</a> that can be used for answering reading comprehension questions or generating new text. And Google is handing out <a href="https://www.protocol.com/bulletins/google-climate-research-grants">grants of up to $100,000 to help battle climate change in a data-driven way</a>.</p>
<p>It comes as no surprise that new machine learning research is therefore published at an increasingly faster rate. ArXiv, a popular public repository for research papers, even registered about 100 machine learning papers being published each day in 2018 (Figure 1). Looking at the exponential growth below, that should be plenty more in 2022.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://timcosemans.be/blog/explainable_cnns/ml_papers.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Machine Learning ArXiv papers per year <span class="citation" data-cites="fournier">(Fournier-Viger 2019)</span></figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-fournier" class="csl-entry">
Fournier-Viger, Philippe. 2019. <span>‘Too Many Machine Learning Papers? | the Data Blog’</span>. <a href="https://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/">https://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/</a>.
</div></div></figure>
</div>
<p>While not all research is directly applicable to an industry context and techniques are not always mature enough to warrant implementation, impactful papers appear almost daily. It’s up to us, the data scientists, to track them down.</p>
<p>In this article, we discuss one of those papers that recently caught our attention. In what follows, we take you along our journey of better understanding how the algorithm works, how to implement it and what it teaches us. More specifically, this article implements the ResNet, a type of convolutional neural network, and applies it to predicting which countries won a medal at the 2022 Winter Olympics. We achieve good performance and, using explainable AI tools, are able to pinpoint why exactly our model performs the way it does.</p>
<section id="deep-learning-for-time-series-classification" class="level1 page-columns page-full">
<h1>Deep learning for time series classification</h1>
<p>Problems in industry are plentiful but often involve some type of binary classification. Will this patient develop cancer? Is this email spam or ham? Will this customer purchase another product? Whether these questions can be answered frequently depends on the type of data available. In the past, companies were often only able to provide data from one point in time. Yet as information systems have become more mature, companies are keeping track of ever-growing amounts of data and are increasingly able to provide data over multiple time periods. They track machine behaviour over time, customer purchases or even employee happiness and want to predict whether a machine will fail, a customer will churn or an employee will leave.</p>
<p>Techniques that are able to make accurate classifications, using these individual level time series as input, are therefore valuable for industry. In this regard, a (fairly) recent review paper <span class="citation" data-cites="ismail">(Ismail Fawaz et al. 2019)</span> caught our attention. As the field of time series classification has gained more traction over the years, a myriad of algorithms has been developed to solve the same problem. Through extensive testing, these authors are able to single out which perform best and under which circumstances. Among them is the residual network (ResNet), a convolutional neural network originally used for image classification.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ismail" class="csl-entry">
Ismail Fawaz, Hassan, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. 2019. <span>‘Deep Learning for Time Series Classification: A Review’</span>. <em>Data Mining and Knowledge Discovery</em> 33 (4): 917–63.
</div></div></section>
<section id="how-it-works-convolutional-neural-networks-101" class="level1 page-columns page-full">
<h1>How it works: Convolutional Neural Networks 101</h1>
<p>At its core, a residual network (ResNet) is a convolutional neural network (CNN). These types of neural networks have already been successfully implemented in image recognition and natural language processing. While this branch of neural networks is therefore most well-known for separating cats from dogs or summarising reviews on TripAdvisor, it also has its applications to time series classification.</p>
<p>The CNN gets its strength from repeatedly sliding a ‘filter’ over a time series (Figure 2), taking some weighted average of the values in its range, and thereby producing a new time series. Different filters will extract different features (such as trends) and applying a filter to an already filtered time series allows for detection of even the best hidden signals. The weights for each of the observations in a filter is determined by training the CNN. They are optimized for classification of the time series at hand.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://timcosemans.be/blog/explainable_cnns/filtering.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Illustration of the filtering process</figcaption>
</figure>
</div>
<section id="the-residual-network" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-residual-network">The Residual Network</h2>
<p>The residual network (ResNet) is a special type of CNN that differs in its architecture (or sequence of layers; Figure 3). While we have done our research on the latter, the ResNet requires special attention as it contains some modifications. More specifically, a residual network <span class="citation" data-cites="he">Wang, Yan, and Oates (2017)</span> contains three blocks of each three convolutional layers (Figure 3). The first block contains three layers of 64 filters, the last two blocks contain 128 filters per layer. After each block of three convolutional layers, the input to that block (i.e., the raw data or the output of the previous block) is added to the output of the third layer, using a so-called ‘shortcut connection’. In addition, after each convolutional layer, batch normalisation is executed. In the last convolutional layer of each block, the input from the shortcut connection is also batch normalised independently before being added. Lastly, after each layer, each of the elements is subjected to a ReLU activation function before being passed on to the next layer.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wang" class="csl-entry">
Wang, Zhiguang, Weizhong Yan, and Tim Oates. 2017. <span>‘Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline’</span>. In <em>2017 International Joint Conference on Neural Networks (IJCNN)</em>, 1578–85. IEEE.
</div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://timcosemans.be/blog/explainable_cnns/architecture.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Architecture of a Residual Network</figcaption>
</figure>
</div>
<p>After three blocks of convolutional layers, the output of the third block is put through a global average pooling (GAP) layer: Each time series is averaged and can now be represented by a single node. In a final stage, each of these nodes is fully connected to the output layer, i.e., the nodes with the class indication. A sigmoid activation function allows for the prediction of a probability for belonging to each of the classes.</p>
</section>
</section>
<section id="how-to-implement-it-application-to-the-2022-olympic-winter-games" class="level1">
<h1>How to implement it: Application to the 2022 Olympic Winter Games</h1>
<p>Now that we’ve seen the theoretical basis behind the ResNet, it’s time for an application! While many machine learning models are implemented in well-known Python libraries, the ResNet for time series classification is not. No need to panic! We will not have to optimise our loss functions by hand, we can just rely on a deep learning framework, such as TensorFlow or PyTorch. To get a feel for the power of the ResNet, we will now try to predict whether a country has won a medal at the Olympic Winter Games in 2022 just by looking at their GDP from 1990 to 2018. The code behind our results can be found on <a href="https://github.com/TimCosemans/explainable-cnns/tree/main">GitHub</a>. For now, we will just focus on the results and what they mean. This section is somewhat technical, you can skip it if you want and just move on to the interpretation of the results.</p>
<p>The data we use was collected from the web (https://olympics.com/en/olympic-games/beijing-2022/medals and https://www.kaggle.com/datasets/nitishabharathi/gdp-per-capita-all-countries) and put into files that can easily be read by Python. We only include countries that participated in the 2022 Olympic Winter Games. In addition, the ResNet requires full time series without missing values. For the purpose of this example, we therefore choose to delete 18 cases with missing values (although there might be better strategies to deal with the missingness). Our data now consists of 71 countries, 50 of which will be used to train the model. The remaining 21 make up the test set, i.e., the unseen observations on which our final model will be evaluated.</p>
<p>The model we built matches the architecture from Figure 3 exactly. Yet however good it may be, without some sense-making it remains a black box. Let’s first start by inspecting its performance. Throughout the epochs (i.e., passes through the entire data set), the model should become better at matching the data. Figure 4 shows that both the training and test loss decline throughout the epochs. After about 350 epochs, our training stops since training loss has not decreased for several epochs. We also plot the test loss, yet do not make decisions based on this quantity. As expected, the test loss is always slightly higher than the training loss. Both training and test accuracy rapidly rise after the first 50 epochs and then remain almost constant at 80-85%. The AUC follows a similar pattern and remains constant at around 0.9. Overall, the model performs well. Since the accuracy of a classification problem is highly dependent on the chosen threshold, we choose our best model based on the training AUC. This is maximal around 210 epochs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://timcosemans.be/blog/explainable_cnns/performance.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: Performance of the model in terms of loss, accuracy, and AUC for both the test and training set</figcaption>
</figure>
</div>
<p>For the best model, we subsequently plot the ROC curve (Figure 5). With an AUC of 0.91 it performs significantly better than a random guess (which would have an AUC of 0.5). We choose the best threshold as the point on the curve that is closest to the upper left corner in terms of Euclidean distance, where the true positive rate is 1 and the false positive rate is 0. This point is indicated with a red dot. At this point, if we classify a country as a medal winner only if its predicted probability is above 0.54, we are able to achieve a false positive rate of 0.09 and a true positive rate of 0.76.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://timcosemans.be/blog/explainable_cnns/roc.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: ROC curve for the model with the best training AUC</figcaption>
</figure>
</div>
<section id="what-it-means-explaining-the-previous-paragraph" class="level2">
<h2 class="anchored" data-anchor-id="what-it-means-explaining-the-previous-paragraph">What it means: Explaining the previous paragraph</h2>
<p>The discussion above was very technical and still does not give us much insight into the power of the ResNet. The model we chose, based on the results from the training set, will only classify a country as a medal winner if the predicted probability it wins one is larger than 54%. To present an unbiased estimate of this model’s performance, we calculate some statistics on the test set. Our model achieves an overall precision of about 85% and a recall of about 87%. This means that 85% of all predicted medal winners will actually win one and we correctly predict 87% of all true medal winners. Given that our model only uses GDP data (ending three years before the 2022 Olympic Winter Games), this is a rather good performance!</p>
<p>Yet you might be wondering how we can achieve such good performance with the data at hand. As with many deep learning models, our model remains a black box. To find out what features of the time series add to its prediction, we can use a class activation map. This tool allows us to determine the importance of each temporal element for the classification of the time series. Our class activation map (Figure 6) shows the importance of the GDP in each year for the probability of that country being a medal winner. It shows that especially periods of high growth contribute to a higher probability of winning a medal at the 2022 Winter Olympics. Intuitively this makes sense: Wealthier countries have more funds to invest in the development of athletic capabilities and are therefore more likely to win a medal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://timcosemans.be/blog/explainable_cnns/cam.png" class="img-fluid figure-img"></p>
<figcaption>Figure 6: Class activation map for the test set (high values are indicated by a deep red, low values by a deep blue)</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Data science is an evolving field and as a data scientists we have to stay in the loop with regards to its latest innovations. This article gives some insight into that process. From researching the foundations of the ResNet to building the neural network and interpreting its results. By presenting the ResNets good performance and interpretable results, we hope to have convinced you of the potential of neural networks for the classification of time series. Happy programming!</p>



</section>

 ]]></description>
  <category>explainable ai</category>
  <category>data science</category>
  <category>neural networks</category>
  <guid>https://timcosemans.be/blog/explainable_cnns/</guid>
  <pubDate>Tue, 07 Jun 2022 00:00:00 GMT</pubDate>
  <media:content url="https://timcosemans.be/blog/explainable_cnns/header.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
