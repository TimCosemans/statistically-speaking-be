[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Teaching Assistant | Hasselt University\n02/2026 – Present\n\nOrganizing practical sessions for the course ‘Explorative and Descriptive Data Analysis’\nGrading assignments and exams\n\nData Scientist & Engineer | Smartschool\n10/2023 – Present\n\nDeveloping AI-driven solutions (predictive modeling and development of an LLM-based assistant using RAG).\nLeading business intelligence initiatives and implementing scalable data infrastructure.\nStarting data governance, quality and risk management initiatives.\nCollaborating with stakeholders to ensure ethical data use in education.\nOrganizing workshops to increase data literacy across the organization.\n\nAdditional Activities:\n\nData Team Lead: Organizing workshops, conducting AI maturity assessments, developing roadmaps and planning.\nAligning AI and data initiatives with the company’s core values and mission.\n\nAffiliate Researcher | Eindhoven University of Technology\n01/2022 – Present\n\nConducting research at the intersection of marketing analytics, behavioral science, and AI.\nExploring predictive models to understand customer behavior and decision-making processes.\n\nData Science Consultant | Algorhythm\n01/2021 – 10/2023\n\nDeveloped machine learning and statistical models to drive business insights across industries.\nLed data-driven marketing strategies, optimizing customer journeys through advanced analytics.\nProvided technical and strategic consulting to help businesses leverage AI effectively.\n\nAdditional Activities:\n\nMarketing Data Science Offering: Developed value propositions, conducted R&D, led client workshops.\nMarketing Strategy Review: Supervised interns, conducted keyword analysis, produced brand video content.\nResearch & Learning Lead: Organized stakeholder meetings, developed internal learning strategies.\nBusiness Unit Strategy: Designed three-year strategic plans, defined KPIs, and implemented tracking systems.\nSales & Business Development: Drove new client acquisition and partnership initiatives.\n\nDoctoral Researcher | Eindhoven University of Technology\n10/2020 – 12/2021\n\nConducted research on marketing analytics and behavioral modeling.\nApplied machine learning and econometrics to study consumer decision-making.\n\nAdditional Activities:\n\nPhD Representative: Guided new students, attended board meetings, organized community events.\n\n\n\n\nMarketing Effectiveness Intern | GfK (07/2019 – 08/2019)\nPolicy Intern | Hasselt University (07/2018 – 08/2018)\n\n\n\n\n\nInternship Supervisions | Hasselt University (2023): Guided graduation projects in marketing strategy & value proposition management.\nThesis Supervision | Thomas More University College (2023): Bachelor thesis on causal machine learning.\nThesis Supervision | Eindhoven University of Technology (2021): Innovation management & marketing analytics projects.\nCourses Taught: Explorative and Descriptive Data Analysis (2026), Multivariate Statistics (2021), Statistics for Business Administration (2019), Mathematics for Business Administration (2016-2019)."
  },
  {
    "objectID": "cv/index.html#internships",
    "href": "cv/index.html#internships",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Marketing Effectiveness Intern | GfK (07/2019 – 08/2019)\nPolicy Intern | Hasselt University (07/2018 – 08/2018)"
  },
  {
    "objectID": "cv/index.html#teaching-supervision",
    "href": "cv/index.html#teaching-supervision",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Internship Supervisions | Hasselt University (2023): Guided graduation projects in marketing strategy & value proposition management.\nThesis Supervision | Thomas More University College (2023): Bachelor thesis on causal machine learning.\nThesis Supervision | Eindhoven University of Technology (2021): Innovation management & marketing analytics projects.\nCourses Taught: Explorative and Descriptive Data Analysis (2026), Multivariate Statistics (2021), Statistics for Business Administration (2019), Mathematics for Business Administration (2016-2019)."
  },
  {
    "objectID": "cv/index.html#certifications",
    "href": "cv/index.html#certifications",
    "title": "Curriculum Vitae",
    "section": "Certifications",
    "text": "Certifications\n\nCronos Leadership Track\nDataTalks MLOps Certification\nDataTalks LLM Certification\nDatabricks Certified Machine Learning Associate\nMicrosoft Certified Azure Data Scientist Associate"
  },
  {
    "objectID": "cv/index.html#technical-skills",
    "href": "cv/index.html#technical-skills",
    "title": "Curriculum Vitae",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nProgramming & Tools: Python, SQL, MLFlow, Airflow, PySpark, Puppet, Logstash, Elastic Search, Kibana, Docker.\nMachine Learning & Analytics: Statistical modeling, predictive analytics, econometrics, optimization, causal inference.\nAI & Business Intelligence: Explainable AI, large language models, AI ethics, data strategy, dashboard development."
  },
  {
    "objectID": "cv/index.html#papers",
    "href": "cv/index.html#papers",
    "title": "Curriculum Vitae",
    "section": "Papers",
    "text": "Papers\n\nBelieve It Before You See It: The Role of Trust in AI Adoption | Work in progress\nStudent Dynamics: A Fair Early Warning System to Detect Dropout | Work in progress\nLoosening the Ties that Bind: Churn and Discontinuance in Multiproduct Settings | Work in progress\nExploratory Graph Analysis for Factor Retention: Simulation Results for Continuous and Binary Data | Educational & Psychological Measurement"
  },
  {
    "objectID": "cv/index.html#projects",
    "href": "cv/index.html#projects",
    "title": "Curriculum Vitae",
    "section": "Projects",
    "text": "Projects\n\nLLM Assistant (Smartschool): Using the Smartschool manual to develop a smart assistant with RAG.\nSmart Signal (Smartschool): Predicting student failure using ML.\nBusiness Intelligence (Smartschool): Implemented BI system using ELK stack.\nAnomaly Detection (dotArchie | Algorhythm): Built neural networks to predict server failures.\nMarketing Data Engineering (Publiq | Algorhythm): Set up website tracking infrastructure.\nEconomic Forecasting (Department of Work | Algorhythm): Developed statistical forecasting model for cost prediction.\nDisease Trajectory Analysis (Johnson & Johnson | Algorhythm): Built statistical analysis dashboards for clinical trials.\nPreventive Maintenance (IPEE | Algorhythm): Developed toilet clog prediction model.\nProduction Optimization (Toyo Ink | Algorhythm): Time series modeling of IoT data.\nCustomer Journey Analysis (Smeg | Algorhythm): Website process mining for customer behavior insights."
  },
  {
    "objectID": "cv/index.html#public-speaking-talks",
    "href": "cv/index.html#public-speaking-talks",
    "title": "Curriculum Vitae",
    "section": "Public Speaking & Talks",
    "text": "Public Speaking & Talks\n\nFaculty of Law, Economics and Finance at University of Luxembourg (2025): Student Dynamics: An Early Warning System to Prevent Student Dropout.\nTrefdag Vlaanderen Digitaal (2024): AI in education & student performance.\nStudiedag AI Digitaal Vlaanderen (2024): Ethical AI in education.\nStudiedag AI Kenniscentrum Digisprong (2024): Critical AI perspectives in education.\nBusiness Club AI Voka (2023): Data-driven marketing analytics.\nData Science Meetup Leuven (2020): Exploratory factor analysis for binary data."
  },
  {
    "objectID": "blog/panel_data_note/index.html",
    "href": "blog/panel_data_note/index.html",
    "title": "A note on panel data methods",
    "section": "",
    "text": "Defining Panel Data\nPanel, or longitudinal data has become widely available to empirical researchers across economics. Many countries conduct national periodic surveys, such as the British Household Panel Survey or the National Longitudinal Survey in the United States. In addition, commercial organizations offer a wide array of longitudinal datasets for research, such as Nielsen’s Consumer Panel. The units of analysis, individuals, firms etc., are not only observed on different variables (like in cross-sectional data), but also over time. Key to this type of data is the fact that the observations, i.e., the individual data points, are no longer independent since they belong to the same unit of analysis. The most outspoken benefit of this type of data lies in this within-unit variance, that brings extra information in addition to the traditional between-unit variance observed in cross-sectional data (Capitaine, Genuer, and Thiébaut 2021).\n\nCapitaine, Louis, Robin Genuer, and Rodolphe Thiébaut. 2021. ‘Random Forests for High-Dimensional Longitudinal Data’. Statistical Methods in Medical Research 30 (1): 166–84.\nPanel or longitudinal data is therefore a special case of clustered data, where data points belong to a cluster that causes a correlation between them. A different type of clustered data is hierarchical data where data points are connected not because they belong to the same unit of analysis, but because the unit of analysis belongs to some overarching group. For example, test scores of students have a hierarchical dimension since the unit of analysis, the student, belongs to a group, a class. All grades from students of the same class can be expected to show some correlation due to characteristics of the class, such as the teacher, the class size etc.\n\n\nAnalyzing Panel Data\nBoth econometrics and statistics have a large array of methods to analyze panel data, but the terminology used to denote them can often be conflicting and confusing (Gelman 2005). In general, we can define a longitudinal model with individual specific intercepts as follows:\n\nGelman, Andrew. 2005. ‘Why i Don’t Use the Term “Fixed and Random Effects” | Statistical Modeling, Causal Inference, and Social Science’. https://statmodeling.stat.columbia.edu/2005/01/25/why_i_dont_use/.\n\\[\nY_{it} = f(X_{it}) + c_i + e_{it}\n\\]\nwhere:\n\n\\(f(X_{it})\\) is an unknown function\n\n\\(Y_{it}\\) represents the outcome\n\n\\(e_{it}\\) is a standard normally distributed error\n\n\\(c_i\\) is an unobserved, time-constant factor specific to individual i\n\nIn econometrics, this model is called the unobserved effects model (Wooldridge 2016). There, the discussion usually centers around whether \\(c_i\\) must be treated as a random variable (i.e., a random effect) or a parameter to be estimated (i.e., a fixed effect).\nOver time:\n\n“Random effects” has become synonymous with zero correlation between \\(c_i\\) and \\(X_i\\)\n\n“Fixed effects” implies correlation between \\(c_i\\) and \\(X_i\\)\n\nIn statistics, on the other hand, a fixed effect is a common term used to refer to any parameter in a regression model that is a population average to be estimated, usually denoted by \\(\\beta\\).\nEstimation under the assumption of non-zero correlation in econometrics (“fixed effects estimation”) is more realistic, but requires the removal of the influence of \\(c_i\\). This can be done using the “within” transformation of the equation to be estimated, which is equivalent to estimating with OLS\n\\[\nY_{it} - Ȳ_i = f(X_{it} - X̄_i) + e_{it} - ē_i\n\\]\nWhile this approach successfully removes the influence of \\(c_i\\), it also removes all time-constant variables. The main downside of this approach is that other time-constant variables cannot be included. Estimation under the assumption of no correlation (“random effects estimation”), does allow for the inclusion of time-constant variables, but is often less realistic. This involves estimating\n\\[\nY_{it} = f(X_{it}) + v_{it}\n\\]\nwhere \\(v_{it} = c_i + e_{it}\\). Econometricians account for the serial correlation introduced in the composite error term by using generalised least squares. Similar approaches exist in statistics to model the influence of individual-specific factors. They all fall under the “generalised linear mixed model” (GLMM). The general linear mixed model can be written as\n\\[\nY_{it} = f(X_{it}) + W_{it} * c_i + e_{it}\n\\]\nWhere:\n\n\\(X_{it}\\) are covariates with fixed effects\n\n\\(W_{it}\\) are covariates with random effects\n\nThese last two equations are equivalent if we only include a random intercept (\\(W_{it} = 1\\)). What the econometrician calls unobserved effects, the statistician calls random intercepts.\nCaution is needed, however. While the econometrician’s random effects model is equivalent to the statistician’s random intercept model, estimating both (e.g., using the plm and lmer packages in R) can yield different results as mixed models are usually estimated using (restricted or unrestricted) maximum likelihood and not generalised least squares.\n“The econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on the machine, the expressions for the estimators are usually rather simple. ML estimation of longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence criteria.” (Croissant and Millo 2008)\n\nCroissant, Yves, and Giovanni Millo. 2008. ‘Panel Data Econometrics in r: The Plm Package’. Journal of Statistical Software 27: 1–43.\nThe GLMM does add the opportunity to also add random, individual specific, slopes for covariates in addition to their general, fixed effect. This can be done by including a variable in both \\(X_{it}\\) and \\(W_{it}\\). These random effects represent random variation in the model that can be attributed to a group to which observations belong. It is with these types of models that opportunities arise to model not only longitudinal but also hierarchical effects simultaneously.\nIn statistics, the econometrician’s worry about violation of the “random effects” assumption still holds when working with mixed models and observational data. There might still be correlation between the error term and the explanatory variables due to other time-constant unobserved effects. Experimental data does not suffer from this type of endogeneity.\nSpecifically for the case of the random intercept, which is the most prevalent in econometrics, we can employ a “correlated random effects approach”, which explicitly models the correlation between \\(X_{it}\\) and \\(c_i\\). This approach assumes\n\\[\nc_i = \\psi + X̄_i * \\phi + a_i\n\\]\nwhere:\n\n\\(X̄_i\\) is the vector of time-averages of all variables\n\\(E(a_i|X_i) = 0\\)\n\nSo that we can estimate:\n\\[\nY_{it} = f(X_{it}) + \\psi + Z_i * \\delta + X̄_i * \\phi + v_{it}\n\\]\nEconometricians again use GLS to deal with serial correlation in the composite error \\(v_{it}\\),all while allowing other time-constant factors and accounting for unobserved time-constant effects that are correlated with \\(X_{it}\\) (Wooldridge 2016).\n\nWooldridge, Jeffrey. 2016. Introductory Econometrics. Cengage AU.\n\n\nFinal Note\nIn addition to the methods described above, econometrics has a different set of methods for analyzing ‘dynamic’ panel data methods, that include lags of variables. I consider these to be outside of the scope of the current discussion."
  },
  {
    "objectID": "blog/ai_nuchterheid/index.html",
    "href": "blog/ai_nuchterheid/index.html",
    "title": "AI-nuchterheid",
    "section": "",
    "text": "Knipper twee keer en je geraakt onherroepelijk achterop in het oerwoud aan nieuwe AI-modellen en -toepassingen. De techgiganten vliegen elkaar de afgelopen maanden onophoudelijk in de haren en dat levert elke week minstens één nieuwe vakterm op die we ons eigen moeten maken. Met de nadruk op moeten, want de beloftes zijn hoog. En je wil de AI-trein toch niet missen?\n\nWaarom doen we dit eigenlijk?\nJe kan het ze niet kwalijk nemen daar in Silicon Valley. De geconcentreerde smeltkroes van ingenieurs en wetenschappers staat te springen voor elke nieuwigheid. Ze zijn ‘innovators’, altijd op zoek naar het nieuwste hoogtechnologische gat in de markt. Hoe groot het gat is, maakt eigenlijk niet uit. Zolang het naar gevuld kan worden. Het is nog maar af te wachten of de zoveelste iteratie van de slimme brillen van Meta, of het nu met AR, VR of AI is, het dit keer wel verder zullen brengen dan de YouTubekanalen van de techfluencers. Wij kunnen niet anders dan ernaar kijken als een kip naar het onweer. Gefascineerd, onder de indruk, en zonder besef van wat er nog komen zal. Maar de Meta’s en de Google’s van deze wereld hebben grotere doelen. Ze mikken hoger. Wat we nu zien zijn slechts bijproducten in de race naar waar het allemaal echt om draait: artificial general intelligence. Of nog beter artificial superintelligence. Het benaderen of zelfs voorbijstreven van de menselijke intelligentie door een machine.\n\n\nKan dat eigenlijk wel?\nTerwijl de psychologie na jaren onderzoek nog steeds geen allesomvattende definitie kan geven van menselijke intelligentie, hebben de computerwetenschappen er alvast een antwoord op: benchmarks. Oftewel vooraf gedefinieerde taken, van gestandaardiseerde examens tot vertalingen, met een duidelijk juist antwoord. Nu ik weet dat onze volledige intellectuele capaciteiten vervat kunnen worden in staafdiagrammen, lijkt psycholoog me opeens geen al te moeilijk beroep meer. Microsoft concludeert daarom in een recent onderzoek dat heel wat banen op de schop kunnen. Wat handig dat zij al even alles op alles zetten om hun AI-assistent, Copilot, aan de man te krijgen. Het lijstje van die beroepen? Dat loopt uiteen van schrijvers tot wiskundigen. Iedereen weet toch dat de beste romans en het meest baanbrekende onderzoek gebeurt door mensen die enkel reeds bestaand werken samenvatten, toch?\n\n\nWat brengt de toekomst dan?\nHet eerste techbedrijf dat de eindmeet in de race om artificial general intelligence of zelfs artificial superintelligence weet te halen kan aandeelhouders een gouden toekomst verzekeren. Hoe die eindmeet er precies uitziet, daar blijven de CEO’s van de eerdergenoemde techbedrijven vaag over. Maar in de filmpjes en interviews die aan de lopende band opduiken zie je hun Amerikaanse ogen fonkelen. Van verwachting of angst, dat weten we helaas nog niet. Bij Sam Altman, de CEO van OpenAI, lijkt het er alvast op dat het dat laatste is. Hij vergelijkt de komst van het, nota bene door zijn eigen bedrijf ontwikkelde, GPT-5 met die van de atoombom. Mark Zuckerberg ziet er dan alweer geen graat in en droomt luidop van een poeslieve superintelligentie, dat gedwee onze saaie taakjes zal uitvoeren. (Iemand interesse in een AI Agent die wel aan je belastingaangifte uitkan?) En dat allemaal berustend op zijn recent samengestelde miljoenenteam van knappe koppen. Wat nog moet blijken is of het amalgaam van de knapste koppen ook het knapste team vormt. Mijn oud professor groepsdynamica zou dat alvast tegenspreken. Wat Mark ook gemakshalve vergeet te vermelden is dat zijn bedrijf de Europese AI Act links laat liggen. Of de technologie poeslief wordt weten we nog niet, maar ethisch ontwikkeld zal hij alvast niet zijn. Laten we niet doen alsof wetgeving de heilige graal is. Zelfs de meer meegaande bedrijven, zoals Google, zijn niet onbekend met het geweer van schouder te wisselen als hen dat een voordeel kan opleveren. Want de politieke macht kan zonder veel problemen de geldtoevoer afknippen, dus laten we vooral zorgen dat we bij hen op een goed blaadje staan. Ook al betekent dat dat we ideologisch twijfelachtige regimes moeten steunen. Zijn dat de bedrijven waar we dagelijks onze Europese bedrijfsgeheimen naartoe pompen omdat we te lui zijn om te zoeken naar de omzet van het afgelopen kwartaal in het jaarlijks financieel rapport?\n\n\nEn nu?\nBegrijp me niet verkeerd. Ik kan de komst van de open-source AI-modellen en de tools die ervoor worden ontwikkeld alleen maar toejuichen. De technologie belooft ons leven alleen maar makkelijker te maken. Ze zou zomaar eens een chatbot kunnen opleveren die de volgende generatie aan leerlingen een antwoord kan geven op de vraag waarom de Westerse landen hun ogen zo lang hebben gesloten voor de oorlog in Gaza. Of beter nog, ze zou je een antwoord kunnen geven op de vraag hoeveel vrije dagen je nog op overschot hebt zonder daarbij Mark op de hoogte te brengen van je vakantieplannen."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tim Cosemans",
    "section": "",
    "text": "I love uncovering the stories hidden in data. By combining analytical techniques with behavioral data, I aim to understand why people make the choices they do and how those patterns shape the world around us.\nWhether it’s predicting student drop-out, optimizing marketing strategies, or exploring decision-making in organizations, I enjoy bridging the gap between numbers and real-life behavior. For me, data isn’t just about models and algorithms—it’s a way to gain deeper insights into human nature.\nBeyond my technical work, I’m passionate about sharing knowledge, whether through teaching, mentoring, or collaborating with stakeholders to make AI and data science more accessible and ethical. At the end of the day, my goal is to use data not just for predictions, but to better understand and improve the way we interact, learn, and grow."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Tim Cosemans",
    "section": "",
    "text": "I love uncovering the stories hidden in data. By combining analytical techniques with behavioral data, I aim to understand why people make the choices they do and how those patterns shape the world around us.\nWhether it’s predicting student drop-out, optimizing marketing strategies, or exploring decision-making in organizations, I enjoy bridging the gap between numbers and real-life behavior. For me, data isn’t just about models and algorithms—it’s a way to gain deeper insights into human nature.\nBeyond my technical work, I’m passionate about sharing knowledge, whether through teaching, mentoring, or collaborating with stakeholders to make AI and data science more accessible and ethical. At the end of the day, my goal is to use data not just for predictions, but to better understand and improve the way we interact, learn, and grow."
  },
  {
    "objectID": "disclaimer/index.html",
    "href": "disclaimer/index.html",
    "title": "Tim Cosemans",
    "section": "",
    "text": "Opinions expressed on this Site are the author’s own in his personal capacity. They do not reflect the views of the current employer or of any organisation, company or board he is associated with."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "AI-nuchterheid\n\n\n\ndata ethics\n\ndata science\n\n\n\nVal niet te snel voor de hype.\n\n\n\n\n\n24 Aug 2025\n\n\nTim Cosemans\n\n\n\n\n\n\n\n\n\n\n\n\nA note on panel data methods\n\n\n\ndata science\n\neconometrics\n\n\n\nStatisticians and econometricians often don’t speak the same language.\n\n\n\n\n\n6 Apr 2025\n\n\nTim Cosemans\n\n\n\n\n\n\n\n\n\n\n\n\nThe struggles of a data marketer\n\n\n\ndata strategy\n\ndata science\n\nmarketing analytics\n\n\n\nHow data marketing has become more complicated than ever.\n\n\n\n\n\n7 Feb 2023\n\n\nTim Cosemans\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable convolutional neural networks for time series classification\n\n\n\nexplainable ai\n\ndata science\n\nneural networks\n\n\n\nUsing class activation maps to uncover which countries win olympic medals.\n\n\n\n\n\n7 Jun 2022\n\n\nTim Cosemans\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/struggles_data_marketeer/index.html",
    "href": "blog/struggles_data_marketeer/index.html",
    "title": "The struggles of a data marketer",
    "section": "",
    "text": "Marketing is traditionally associated with advertising but is so much more than deciding on which ads to present to your customers. As a marketer, you’re expected to help shape your company’s value proposition, communicate it to your customers and manage their acquisition, development, and churn. Yet this process is not without its problems: Your customers might leave your company; you might face high shopping cart abandonment or have to deal with low conversion rates. As a marketer you will most likely turn to your data to see what is going on. And evidence suggests you should: Reports from McKinsey show that data-driven sales can increase EBITDA by 15 to 25 percent (Böringer et al. 2022).\n\nBöringer, J., A. Dierks, I. Huber, and D. Spillecke. 2022. ‘Insights to Impact: Creating and Sustaining Data-Driven Commercial Growth.’ McKinsey & Company. ttps://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/insights-to-impact-creating-and-sustaining-data-driven-commercial-growth.\nYet your data analysis process can take too long. You have to start from scratch collecting data each time or your analysis methods cannot seem to uncover what is truly happening among your customers. A recent study by the Chief Marketing Officer Council (2022) in partnership with GfK, one of the world’s largest marketing research organisations, shows you are not alone:\n\n“80% Of marketers say data, analytics and insights are very important to winning and retaining customers. Yet nearly two-thirds of those marketers are only moderately confident (or worse) in their data, analytics, and insights systems.” (‘The High-Velocity Data Marketer’ 2022)\n\nData marketing relies on access to all relevant data in order to find answers to business questions. Data access comes with its own barriers however (Figure 1). Often marketeers find themselves unable to integrate all data due to insufficient tools or technology. The files coming from your company database might not be compatible with your preferred analytics software or they might be too large to handle on your personal computer. Moving all your data to a cloud provider, such as Microsoft, Google or Amazon can be a great help in such cases. In choosing such a provider, it is important to consider what needs you need fulfilled. Want to make nice looking dashboards in PowerBI? Microsoft Azure might be the way to go. Want to easily access your data from Google Analytics? Think about Google Cloud Platform as your provider.\n\n\n\nFigure 1: Main barriers to data access (‘The High-Velocity Data Marketer’ 2022)\n\n\nMaking such a decision is however not easy, and your company’s situation might have other requirements. In addition, lack of proper data management processes and scattered data control make it even harder for marketers to gather all their data in one place. Before you make cloud decisions, it is therefore always a good idea to conduct an analytical maturity assessment. What data does your company have? Where is it situated? How do you plan on using it? And what should the results look like?\n\n“81% Of marketers say hiring more talent with AI skills is the most critical need to developing their AI capabilities. Yet hiring more talent is also their biggest AI-related challenge.” (‘The High-Velocity Data Marketer’ 2022)\n\nThe customer journey has become more digitalised throughout the years: Customers do their research online before even meeting a salesperson. These buyer intent signals are critical in understanding the customer’s decision process and its critical moments.\nBuyer intent signals can come from multiple sources, including your website, social media, or internal CRM systems. True competitive advantage comes from generating real-time insights, identifying sudden shifts in these signals, and swiftly adapting and acting on them. This can mean searching for and addressing bottle necks in your customer’s journey or adapting your messages to your customers. At Algorhythm we addressed low engagement among customers of a large pharmaceutical player by building models that provide customer level recommendations on which channels to use, frequency to interact, and content to communicate, for example.\nAs the noise across all of these channels grows louder, data marketers have to work even harder to find, extract and analyse relevant signals and produce actionable insights all while keeping the limited time span of the customer’s interactions with the company in mind. Data marketers therefore need the capabilities to detect opportunities and decide on the next best action while the customer is still engaged with the organisation. The banking sector, for example, often struggles with preventing customer churn. For our client, a large Belgian bank, we built a model to predict not only the propensity of churn but also the best possible offer to address each client with.\nAI has disrupted the standard toolkit of the data marketeer but also given us new tools to uncover even the most well-hidden patterns. The same study by GfK (Figure 2) shows that capabilities like predictive and prescriptive analytics or sentiment analysis are however still out of most marketers’ reach. The abundance of technologies and methods might be overwhelming, but all serve their individual purposes. It is therefore important to ask yourself the right questions before getting started with any of them. Do you want to know which customers will churn within a given time frame? Then you might want to focus on predictive analytics. But do you want to know what type of campaigns you should run in order to attract new customers? Then prescriptive analytics is probably more suited for your purposes. Even scraping online sources to reveal customer sentiment about your newly launched products or ad campaigns before they are reflected in the company’s financials is no longer out of reach with AI.\n\n“The challenge is choosing the right solution from a sea of options.” (‘The High-Velocity Data Marketer’ 2022)\n\nThe struggles of a data marketer are real. And as data becomes more ubiquitous and AI more pervasive, they are not likely to become any less prevalent.\n\n\n\nFigure 2: Data capabilities still out of reach for marketers (‘The High-Velocity Data Marketer’ 2022)\n\n‘The High-Velocity Data Marketer’. 2022. CMO Council. https://www.cmocouncil.org/thought-leadership/reports/the-high-velocity-data-marketer."
  },
  {
    "objectID": "blog/explainable_cnns/index.html",
    "href": "blog/explainable_cnns/index.html",
    "title": "Explainable convolutional neural networks for time series classification",
    "section": "",
    "text": "Data science is a rapidly evolving field. Daily, academics at universities work on the next big algorithm that will help make our lives easier. Large corporations, such as Microsoft and Amazon, have even made in-house research part of their competitive advantage by founding their own institutes covering research in areas ranging from economics and health to human-computer interaction and machine learning. Meta, previously Facebook, recently opened up access to a 175-billion-parameter language model that can be used for answering reading comprehension questions or generating new text. And Google is handing out grants of up to $100,000 to help battle climate change in a data-driven way.\nIt comes as no surprise that new machine learning research is therefore published at an increasingly faster rate. ArXiv, a popular public repository for research papers, even registered about 100 machine learning papers being published each day in 2018 (Figure 1). Looking at the exponential growth below, that should be plenty more in 2022.\nWhile not all research is directly applicable to an industry context and techniques are not always mature enough to warrant implementation, impactful papers appear almost daily. It’s up to us, the data scientists, to track them down.\nIn this article, we discuss one of those papers that recently caught our attention. In what follows, we take you along our journey of better understanding how the algorithm works, how to implement it and what it teaches us. More specifically, this article implements the ResNet, a type of convolutional neural network, and applies it to predicting which countries won a medal at the 2022 Winter Olympics. We achieve good performance and, using explainable AI tools, are able to pinpoint why exactly our model performs the way it does."
  },
  {
    "objectID": "blog/explainable_cnns/index.html#the-residual-network",
    "href": "blog/explainable_cnns/index.html#the-residual-network",
    "title": "Explainable convolutional neural networks for time series classification",
    "section": "The Residual Network",
    "text": "The Residual Network\nThe residual network (ResNet) is a special type of CNN that differs in its architecture (or sequence of layers; Figure 3). While we have done our research on the latter, the ResNet requires special attention as it contains some modifications. More specifically, a residual network Wang, Yan, and Oates (2017) contains three blocks of each three convolutional layers (Figure 3). The first block contains three layers of 64 filters, the last two blocks contain 128 filters per layer. After each block of three convolutional layers, the input to that block (i.e., the raw data or the output of the previous block) is added to the output of the third layer, using a so-called ‘shortcut connection’. In addition, after each convolutional layer, batch normalisation is executed. In the last convolutional layer of each block, the input from the shortcut connection is also batch normalised independently before being added. Lastly, after each layer, each of the elements is subjected to a ReLU activation function before being passed on to the next layer.\n\nWang, Zhiguang, Weizhong Yan, and Tim Oates. 2017. ‘Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline’. In 2017 International Joint Conference on Neural Networks (IJCNN), 1578–85. IEEE.\n\n\n\nFigure 3: Architecture of a Residual Network\n\n\nAfter three blocks of convolutional layers, the output of the third block is put through a global average pooling (GAP) layer: Each time series is averaged and can now be represented by a single node. In a final stage, each of these nodes is fully connected to the output layer, i.e., the nodes with the class indication. A sigmoid activation function allows for the prediction of a probability for belonging to each of the classes."
  },
  {
    "objectID": "blog/explainable_cnns/index.html#what-it-means-explaining-the-previous-paragraph",
    "href": "blog/explainable_cnns/index.html#what-it-means-explaining-the-previous-paragraph",
    "title": "Explainable convolutional neural networks for time series classification",
    "section": "What it means: Explaining the previous paragraph",
    "text": "What it means: Explaining the previous paragraph\nThe discussion above was very technical and still does not give us much insight into the power of the ResNet. The model we chose, based on the results from the training set, will only classify a country as a medal winner if the predicted probability it wins one is larger than 54%. To present an unbiased estimate of this model’s performance, we calculate some statistics on the test set. Our model achieves an overall precision of about 85% and a recall of about 87%. This means that 85% of all predicted medal winners will actually win one and we correctly predict 87% of all true medal winners. Given that our model only uses GDP data (ending three years before the 2022 Olympic Winter Games), this is a rather good performance!\nYet you might be wondering how we can achieve such good performance with the data at hand. As with many deep learning models, our model remains a black box. To find out what features of the time series add to its prediction, we can use a class activation map. This tool allows us to determine the importance of each temporal element for the classification of the time series. Our class activation map (Figure 6) shows the importance of the GDP in each year for the probability of that country being a medal winner. It shows that especially periods of high growth contribute to a higher probability of winning a medal at the 2022 Winter Olympics. Intuitively this makes sense: Wealthier countries have more funds to invest in the development of athletic capabilities and are therefore more likely to win a medal.\n\n\n\nFigure 6: Class activation map for the test set (high values are indicated by a deep red, low values by a deep blue)"
  }
]