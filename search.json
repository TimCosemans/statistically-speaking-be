[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "The struggles of a data marketer\n\n\n\n\n\n\ndata strategy\n\n\ndata science\n\n\nmarketing analytics\n\n\n\nHow data marketing has become more complicated than ever.\n\n\n\n\n\n7 Feb 2023\n\n\nTim Cosemans\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable convolutional neural networks for time series classification\n\n\n\n\n\n\nexplainable ai\n\n\ndata science\n\n\nneural networks\n\n\n\nUsing class activation maps to uncover which countries win olympic medals.\n\n\n\n\n\n7 Jun 2022\n\n\nTim Cosemans\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/struggles_data_marketeer/index.html",
    "href": "blog/struggles_data_marketeer/index.html",
    "title": "The struggles of a data marketer",
    "section": "",
    "text": "Marketing is traditionally associated with advertising but is so much more than deciding on which ads to present to your customers. As a marketer, you’re expected to help shape your company’s value proposition, communicate it to your customers and manage their acquisition, development, and churn. Yet this process is not without its problems: Your customers might leave your company; you might face high shopping cart abandonment or have to deal with low conversion rates. As a marketer you will most likely turn to your data to see what is going on. And evidence suggests you should: Reports from McKinsey show that data-driven sales can increase EBITDA by 15 to 25 percent (Böringer et al. 2022).\n\nBöringer, J., A. Dierks, I. Huber, and D. Spillecke. 2022. ‘Insights to Impact: Creating and Sustaining Data-Driven Commercial Growth.’ McKinsey & Company. ttps://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/insights-to-impact-creating-and-sustaining-data-driven-commercial-growth.\nYet your data analysis process can take too long. You have to start from scratch collecting data each time or your analysis methods cannot seem to uncover what is truly happening among your customers. A recent study by the Chief Marketing Officer Council (2022) in partnership with GfK, one of the world’s largest marketing research organisations, shows you are not alone:\n\n“80% Of marketers say data, analytics and insights are very important to winning and retaining customers. Yet nearly two-thirds of those marketers are only moderately confident (or worse) in their data, analytics, and insights systems.” (‘The High-Velocity Data Marketer’ 2022)\n\nData marketing relies on access to all relevant data in order to find answers to business questions. Data access comes with its own barriers however (Figure 1). Often marketeers find themselves unable to integrate all data due to insufficient tools or technology. The files coming from your company database might not be compatible with your preferred analytics software or they might be too large to handle on your personal computer. Moving all your data to a cloud provider, such as Microsoft, Google or Amazon can be a great help in such cases. In choosing such a provider, it is important to consider what needs you need fulfilled. Want to make nice looking dashboards in PowerBI? Microsoft Azure might be the way to go. Want to easily access your data from Google Analytics? Think about Google Cloud Platform as your provider.\n\n\n\nFigure 1: Main barriers to data access (‘The High-Velocity Data Marketer’ 2022)\n\n\nMaking such a decision is however not easy, and your company’s situation might have other requirements. In addition, lack of proper data management processes and scattered data control make it even harder for marketers to gather all their data in one place. Before you make cloud decisions, it is therefore always a good idea to conduct an analytical maturity assessment. What data does your company have? Where is it situated? How do you plan on using it? And what should the results look like?\n\n“81% Of marketers say hiring more talent with AI skills is the most critical need to developing their AI capabilities. Yet hiring more talent is also their biggest AI-related challenge.” (‘The High-Velocity Data Marketer’ 2022)\n\nThe customer journey has become more digitalised throughout the years: Customers do their research online before even meeting a salesperson. These buyer intent signals are critical in understanding the customer’s decision process and its critical moments.\nBuyer intent signals can come from multiple sources, including your website, social media, or internal CRM systems. True competitive advantage comes from generating real-time insights, identifying sudden shifts in these signals, and swiftly adapting and acting on them. This can mean searching for and addressing bottle necks in your customer’s journey or adapting your messages to your customers. At Algorhythm we addressed low engagement among customers of a large pharmaceutical player by building models that provide customer level recommendations on which channels to use, frequency to interact, and content to communicate, for example.\nAs the noise across all of these channels grows louder, data marketers have to work even harder to find, extract and analyse relevant signals and produce actionable insights all while keeping the limited time span of the customer’s interactions with the company in mind. Data marketers therefore need the capabilities to detect opportunities and decide on the next best action while the customer is still engaged with the organisation. The banking sector, for example, often struggles with preventing customer churn. For our client, a large Belgian bank, we built a model to predict not only the propensity of churn but also the best possible offer to address each client with.\nAI has disrupted the standard toolkit of the data marketeer but also given us new tools to uncover even the most well-hidden patterns. The same study by GfK (Figure 2) shows that capabilities like predictive and prescriptive analytics or sentiment analysis are however still out of most marketers’ reach. The abundance of technologies and methods might be overwhelming, but all serve their individual purposes. It is therefore important to ask yourself the right questions before getting started with any of them. Do you want to know which customers will churn within a given time frame? Then you might want to focus on predictive analytics. But do you want to know what type of campaigns you should run in order to attract new customers? Then prescriptive analytics is probably more suited for your purposes. Even scraping online sources to reveal customer sentiment about your newly launched products or ad campaigns before they are reflected in the company’s financials is no longer out of reach with AI.\n\n“The challenge is choosing the right solution from a sea of options.” (‘The High-Velocity Data Marketer’ 2022)\n\nThe struggles of a data marketer are real. And as data becomes more ubiquitous and AI more pervasive, they are not likely to become any less prevalent.\n\n\n\nFigure 2: Data capabilities still out of reach for marketers (‘The High-Velocity Data Marketer’ 2022)\n\n‘The High-Velocity Data Marketer’. 2022. CMO Council. https://www.cmocouncil.org/thought-leadership/reports/the-high-velocity-data-marketer."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Data Scientist & Engineer | Smartschool\n10/2023 – Present\n\nDeveloping AI-driven solutions to improve student outcomes, including predictive modeling for drop-out prevention.\nLeading business intelligence initiatives and implementing scalable data infrastructure.\nCollaborating with stakeholders to ensure ethical AI deployment in education.\n\nAdditional Activities:\n\nData Team Strategy: Organizing workshops, conducting AI maturity assessments, developing strategic roadmaps.\nAligning AI and data initiatives with the company’s core values and mission.\n\nAffiliate Researcher | Eindhoven University of Technology\n01/2022 – Present\n\nConducting research at the intersection of marketing analytics, behavioral science, and AI.\nExploring predictive models to understand customer behavior and decision-making processes.\n\nData Science Consultant | Algorhythm\n01/2021 – 10/2023\n\nDeveloped machine learning and statistical models to drive business insights across industries.\nLed data-driven marketing strategies, optimizing customer journeys through advanced analytics.\nProvided technical and strategic consulting to help businesses leverage AI effectively.\n\nAdditional Activities:\n\nMarketing Data Science Offering: Developed value propositions, conducted R&D, led client workshops.\nMarketing Strategy Review: Supervised interns, conducted keyword analysis, produced brand video content.\nResearch & Learning Lead: Organized stakeholder meetings, developed internal learning strategies.\nBusiness Unit Strategy: Designed three-year strategic plans, defined KPIs, and implemented tracking systems.\nSales & Business Development: Drove new client acquisition and partnership initiatives.\n\nDoctoral Researcher | Eindhoven University of Technology\n10/2020 – 12/2021\n\nConducted research on marketing analytics and behavioral modeling.\nApplied machine learning and econometrics to study consumer decision-making.\n\nAdditional Activities:\n\nPhD Representative: Guided new students, attended board meetings, organized community events.\n\n\n\n\nMarketing Effectiveness Intern | GfK (07/2019 – 08/2019)\nPolicy Intern | Hasselt University (07/2018 – 08/2018)\n\n\n\n\n\nInternship Supervisions | Hasselt University (2023): Guided graduation projects in marketing strategy & value proposition management.\nThesis Supervision | Thomas More University College (2023): Bachelor thesis on causal machine learning.\nThesis Supervision | Eindhoven University of Technology (2021): Innovation management & marketing analytics projects.\nCourses Taught: Multivariate Statistics, Statistics for Business Administration, Mathematics for Business Administration."
  },
  {
    "objectID": "cv/index.html#internships",
    "href": "cv/index.html#internships",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Marketing Effectiveness Intern | GfK (07/2019 – 08/2019)\nPolicy Intern | Hasselt University (07/2018 – 08/2018)"
  },
  {
    "objectID": "cv/index.html#teaching-supervision",
    "href": "cv/index.html#teaching-supervision",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Internship Supervisions | Hasselt University (2023): Guided graduation projects in marketing strategy & value proposition management.\nThesis Supervision | Thomas More University College (2023): Bachelor thesis on causal machine learning.\nThesis Supervision | Eindhoven University of Technology (2021): Innovation management & marketing analytics projects.\nCourses Taught: Multivariate Statistics, Statistics for Business Administration, Mathematics for Business Administration."
  },
  {
    "objectID": "cv/index.html#certifications",
    "href": "cv/index.html#certifications",
    "title": "Curriculum Vitae",
    "section": "Certifications",
    "text": "Certifications\n\nCronos Leadership Track\nDataTalks MLOps Certification\nDatabricks Certified Machine Learning Associate\nMicrosoft Certified Azure Data Scientist Associate"
  },
  {
    "objectID": "cv/index.html#technical-skills",
    "href": "cv/index.html#technical-skills",
    "title": "Curriculum Vitae",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nProgramming & Tools: Python, SQL, MLFlow, Airflow, PySpark, Puppet, Logstash, Elastic Search, Kibana.\nMachine Learning & Analytics: Statistical modeling, predictive analytics, econometrics, optimization, causal inference.\nAI & Business Intelligence: Explainable AI, AI ethics, data strategy, dashboard development."
  },
  {
    "objectID": "cv/index.html#papers",
    "href": "cv/index.html#papers",
    "title": "Curriculum Vitae",
    "section": "Papers",
    "text": "Papers\n\nSmart Signal: An Early Warning System to Prevent Student Drop-Out | Work in progress\nLoosening the Ties that Bind: Churn and Discontinuance in Multiproduct Settings | Work in progress\nExploratory Graph Analysis for Factor Retention: Simulation Results for Continuous and Binary Data | Educational & Psychological Measurement"
  },
  {
    "objectID": "cv/index.html#projects",
    "href": "cv/index.html#projects",
    "title": "Curriculum Vitae",
    "section": "Projects",
    "text": "Projects\n\nSmart Signal (Smartschool): Predicting student drop-out using explainable ML, field testing adoption strategies.\nBusiness Intelligence Logging (Smartschool): Implemented BI system using ELK stack.\nAnomaly Detection (dotArchie | Algorhythm): Built neural networks to predict server failures.\nMarketing Data Engineering (Publiq | Algorhythm): Set up website tracking infrastructure.\nEconomic Forecasting (Department of Work | Algorhythm): Developed statistical forecasting model for cost prediction.\nDisease Trajectory Analysis (Johnson & Johnson | Algorhythm): Built statistical analysis dashboards for clinical trials.\nPreventive Maintenance (IPEE | Algorhythm): Developed toilet clog prediction model.\nProduction Optimization (Toyo Ink | Algorhythm): Time series modeling of IoT data.\nCustomer Journey Analysis (Smeg | Algorhythm): Website process mining for customer behavior insights."
  },
  {
    "objectID": "cv/index.html#public-speaking-talks",
    "href": "cv/index.html#public-speaking-talks",
    "title": "Curriculum Vitae",
    "section": "Public Speaking & Talks",
    "text": "Public Speaking & Talks\n\nTrefdag Vlaanderen Digitaal (2024): AI in education & student performance.\nStudiedag AI Digitaal Vlaanderen (2024): Ethical AI in education.\nStudiedag AI Kenniscentrum Digisprong (2024): Critical AI perspectives in education.\nBusiness Club AI Voka (2023): Data-driven marketing analytics.\nData Science Meetup Leuven (2020): Exploratory factor analysis for binary data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tim Cosemans",
    "section": "",
    "text": "I love uncovering the stories hidden in data. By combining analytical techniques with behavioral data, I aim to understand why people make the choices they do and how those patterns shape the world around us.\nWhether it’s predicting student drop-out, optimizing marketing strategies, or exploring decision-making in organizations, I enjoy bridging the gap between numbers and real-life behavior. For me, data isn’t just about models and algorithms—it’s a way to gain deeper insights into human nature.\nBeyond my technical work, I’m passionate about sharing knowledge, whether through teaching, mentoring, or collaborating with stakeholders to make AI and data science more accessible and ethical. At the end of the day, my goal is to use data not just for predictions, but to better understand and improve the way we interact, learn, and grow."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Tim Cosemans",
    "section": "",
    "text": "I love uncovering the stories hidden in data. By combining analytical techniques with behavioral data, I aim to understand why people make the choices they do and how those patterns shape the world around us.\nWhether it’s predicting student drop-out, optimizing marketing strategies, or exploring decision-making in organizations, I enjoy bridging the gap between numbers and real-life behavior. For me, data isn’t just about models and algorithms—it’s a way to gain deeper insights into human nature.\nBeyond my technical work, I’m passionate about sharing knowledge, whether through teaching, mentoring, or collaborating with stakeholders to make AI and data science more accessible and ethical. At the end of the day, my goal is to use data not just for predictions, but to better understand and improve the way we interact, learn, and grow."
  },
  {
    "objectID": "disclaimer/index.html",
    "href": "disclaimer/index.html",
    "title": "Tim Cosemans",
    "section": "",
    "text": "Opinions expressed on this Site are the author’s own in his personal capacity. They do not reflect the views of the current employer or of any organisation, company or board he is associated with."
  },
  {
    "objectID": "blog/explainable_cnns/index.html",
    "href": "blog/explainable_cnns/index.html",
    "title": "Explainable convolutional neural networks for time series classification",
    "section": "",
    "text": "Data science is a rapidly evolving field. Daily, academics at universities work on the next big algorithm that will help make our lives easier. Large corporations, such as Microsoft and Amazon, have even made in-house research part of their competitive advantage by founding their own institutes covering research in areas ranging from economics and health to human-computer interaction and machine learning. Meta, previously Facebook, recently opened up access to a 175-billion-parameter language model that can be used for answering reading comprehension questions or generating new text. And Google is handing out grants of up to $100,000 to help battle climate change in a data-driven way.\nIt comes as no surprise that new machine learning research is therefore published at an increasingly faster rate. ArXiv, a popular public repository for research papers, even registered about 100 machine learning papers being published each day in 2018 (Figure 1). Looking at the exponential growth below, that should be plenty more in 2022.\nWhile not all research is directly applicable to an industry context and techniques are not always mature enough to warrant implementation, impactful papers appear almost daily. It’s up to us, the data scientists, to track them down.\nIn this article, we discuss one of those papers that recently caught our attention. In what follows, we take you along our journey of better understanding how the algorithm works, how to implement it and what it teaches us. More specifically, this article implements the ResNet, a type of convolutional neural network, and applies it to predicting which countries won a medal at the 2022 Winter Olympics. We achieve good performance and, using explainable AI tools, are able to pinpoint why exactly our model performs the way it does."
  },
  {
    "objectID": "blog/explainable_cnns/index.html#the-residual-network",
    "href": "blog/explainable_cnns/index.html#the-residual-network",
    "title": "Explainable convolutional neural networks for time series classification",
    "section": "The Residual Network",
    "text": "The Residual Network\nThe residual network (ResNet) is a special type of CNN that differs in its architecture (or sequence of layers; Figure 3). While we have done our research on the latter, the ResNet requires special attention as it contains some modifications. More specifically, a residual network Wang, Yan, and Oates (2017) contains three blocks of each three convolutional layers (Figure 3). The first block contains three layers of 64 filters, the last two blocks contain 128 filters per layer. After each block of three convolutional layers, the input to that block (i.e., the raw data or the output of the previous block) is added to the output of the third layer, using a so-called ‘shortcut connection’. In addition, after each convolutional layer, batch normalisation is executed. In the last convolutional layer of each block, the input from the shortcut connection is also batch normalised independently before being added. Lastly, after each layer, each of the elements is subjected to a ReLU activation function before being passed on to the next layer.\n\nWang, Zhiguang, Weizhong Yan, and Tim Oates. 2017. ‘Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline’. In 2017 International Joint Conference on Neural Networks (IJCNN), 1578–85. IEEE.\n\n\n\nFigure 3: Architecture of a Residual Network\n\n\nAfter three blocks of convolutional layers, the output of the third block is put through a global average pooling (GAP) layer: Each time series is averaged and can now be represented by a single node. In a final stage, each of these nodes is fully connected to the output layer, i.e., the nodes with the class indication. A sigmoid activation function allows for the prediction of a probability for belonging to each of the classes."
  },
  {
    "objectID": "blog/explainable_cnns/index.html#what-it-means-explaining-the-previous-paragraph",
    "href": "blog/explainable_cnns/index.html#what-it-means-explaining-the-previous-paragraph",
    "title": "Explainable convolutional neural networks for time series classification",
    "section": "What it means: Explaining the previous paragraph",
    "text": "What it means: Explaining the previous paragraph\nThe discussion above was very technical and still does not give us much insight into the power of the ResNet. The model we chose, based on the results from the training set, will only classify a country as a medal winner if the predicted probability it wins one is larger than 54%. To present an unbiased estimate of this model’s performance, we calculate some statistics on the test set. Our model achieves an overall precision of about 85% and a recall of about 87%. This means that 85% of all predicted medal winners will actually win one and we correctly predict 87% of all true medal winners. Given that our model only uses GDP data (ending three years before the 2022 Olympic Winter Games), this is a rather good performance!\nYet you might be wondering how we can achieve such good performance with the data at hand. As with many deep learning models, our model remains a black box. To find out what features of the time series add to its prediction, we can use a class activation map. This tool allows us to determine the importance of each temporal element for the classification of the time series. Our class activation map (Figure 6) shows the importance of the GDP in each year for the probability of that country being a medal winner. It shows that especially periods of high growth contribute to a higher probability of winning a medal at the 2022 Winter Olympics. Intuitively this makes sense: Wealthier countries have more funds to invest in the development of athletic capabilities and are therefore more likely to win a medal.\n\n\n\nFigure 6: Class activation map for the test set (high values are indicated by a deep red, low values by a deep blue)"
  },
  {
    "objectID": "blog/panel_data_note.html",
    "href": "blog/panel_data_note.html",
    "title": "A Note on Panel Data Methods",
    "section": "",
    "text": "Panel, or longitudinal data has become widely available to empirical researchers across economics. Many countries conduct national periodic surveys, such as the British Household Panel Survey or the National Longitudinal Survey in the United States. In addition, commercial organizations offer a wide array of longitudinal datasets for research, such as Nielsen’s Consumer Panel. The units of analysis, individuals, firms etc., are not only observed on different variables (like in cross-sectional data), but also over time. Key to this type of data is the fact that the observations, i.e., the individual data points, are no longer independent since they belong to the same unit of analysis. The most outspoken benefit of this type of data lies in this within-unit variance, that brings extra information in addition to the traditional between-unit variance observed in cross-sectional data (Capitaine, Genuer & Thiébaut, 2021).\nPanel or longitudinal data is therefore a special case of clustered data, where data points belong to a cluster that causes a correlation between them. A different type of clustered data is hierarchical data where data points are connected not because they belong to the same unit of analysis, but because the unit of analysis belongs to some overarching group. For example, test scores of students have a hierarchical dimension since the unit of analysis, the student, belongs to a group, a class. All grades from students of the same class can be expected to show some correlation due to characteristics of the class, such as the teacher, the class size etc.\n\n\n\nBoth econometrics and statistics have a large array of methods to analyze panel data, but the terminology used to denote them can often be conflicting and confusing (Gelman, 2005). In general, we can define a longitudinal model with individual specific intercepts as follows:\nY_{it} = f(X_{it}) + c_i + e_{it}\nwhere:\n\n\\(f(X_{it})\\) is an unknown function\n\n\\(Y_{it}\\) represents the outcome\n\n\\(e_{it}\\) is a standard normally distributed error\n\n\\(c_i\\) is an unobserved, time-constant factor specific to individual i\n\nIn econometrics, this model is called the unobserved effects model (Wooldridge, 2010). There, the discussion usually centers around whether ci must be treated as a random variable (i.e., a random effect) or a parameter to be estimated (i.e., a fixed effect).\nOver time:\n\n“Random effects” has become synonymous with zero correlation between \\(c_i\\) and \\(X_i\\)\n\n“Fixed effects” implies correlation between \\(c_i\\) and \\(X_i\\)\n\nIn statistics, on the other hand, a fixed effect is a common term used to refer to any parameter in a regression model that is a population average to be estimated, usually denoted by \\(\\beta\\) (StackExchange, 2012b).\nEstimation under the assumption of non-zero correlation in econometrics (“fixed effects estimation”) is more realistic, but requires the removal of the influence of \\(c_i\\). This can be done using the “within” transformation of the equation to be estimated, which is equivalent to estimating with OLS\nY_{it} - Ȳ_i = f(X_{it} - X̄_i) + e_{it} - ē_i\nWhile this approach successfully removes the influence of \\(c_i\\), it also removes all time-constant variables. The main downside of this approach is that other time-constant variables cannot be included. Estimation under the assumption of no correlation (“random effects estimation”), does allow for the inclusion of time-constant variables, but is often less realistic. This involves estimating\nY_{it} = f(X_{it}) + v_{it}\nwhere \\(v_{it} = c_i + e_{it}\\). Econometricians account for the serial correlation introduced in the composite error term by using generalised least squares. Similar approaches exist in statistics to model the influence of individual-specific factors. They all fall under the “generalised linear mixed model” (GLMM). The general linear mixed model can be written as\nY_{it} = f(X_{it}) + W_{it} * c_i + e_{it}\nWhere:\n\n\\(X_{it}\\) are covariates with fixed effects\n\n\\(W_{it}\\) are covariates with random effects\n\nThese last two equations are equivalent if we only include a random intercept (\\(W_{it} = 1\\)). What the econometrician calls unobserved effects, the statistician calls random intercepts (StackExchange, 2012b).\nCaution is needed, however. While the econometrician’s random effects model is equivalent to the statistician’s random intercept model, estimating both (e.g., using the plm and lmer packages in R) can yield different results as mixed models are usually estimated using (restricted or unrestricted) maximum likelihood and not generalised least squares (StackExchange, 2016).\n“The econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on the machine, the expressions for the estimators are usually rather simple. ML estimation of longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence criteria.” (Croissant & Millo, 2008, p.34)\nThe GLMM does add the opportunity to also add random, individual specific, slopes for covariates in addition to their general, fixed effect. This can be done by including a variable in both Xit and Wit (Zablotski, 2019c). These random effects represent random variation in the model that can be attributed to a group to which observations belong (Zablotski, 2019a). It is with these types of models that opportunities arise to model not only longitudinal but also hierarchical effects simultaneously (Zablotski, 2019b).\nIn statistics, the econometrician’s worry about violation of the “random effects” assumption still holds when working with mixed models and observational data (StackExchange, 2012a). There might still be correlation between the error term and the explanatory variables due to other time-constant unobserved effects. Experimental data does not suffer from this type of endogeneity.\nSpecifically for the case of the random intercept, which is the most prevalent in econometrics, we can employ a “correlated random effects approach”, which explicitly models the correlation between \\(X_{it}\\) and \\(c_i\\). This approach assumes\nc_i = \\psi + X̄_i * \\phi + a_i\nwhere:\n\n\\(X̄_i\\) is the vector of time-averages of all variables\n$E(a_i|X_i) = 0\n\nSo that we can estimate:\nY_{it} = f(X_{it}) + \\psi + Z_i * \\delta + X̄_i * \\phi + v_{it}\nEconometricians again use GLS to deal with serial correlation in the composite error \\(v_{it}\\),all while allowing other time-constant factors and accounting for unobserved time-constant effects that are correlated with \\(X_{it}\\) (Wooldridge, 2010).\n\n\n\nIn addition to the methods described above, econometrics has a different set of methods for analyzing ‘dynamic’ panel data methods, that include lags of variables. I consider these to be outside of the scope of the current discussion."
  },
  {
    "objectID": "blog/panel_data_note.html#defining-panel-data",
    "href": "blog/panel_data_note.html#defining-panel-data",
    "title": "A Note on Panel Data Methods",
    "section": "",
    "text": "Panel, or longitudinal data has become widely available to empirical researchers across economics. Many countries conduct national periodic surveys, such as the British Household Panel Survey or the National Longitudinal Survey in the United States. In addition, commercial organizations offer a wide array of longitudinal datasets for research, such as Nielsen’s Consumer Panel. The units of analysis, individuals, firms etc., are not only observed on different variables (like in cross-sectional data), but also over time. Key to this type of data is the fact that the observations, i.e., the individual data points, are no longer independent since they belong to the same unit of analysis. The most outspoken benefit of this type of data lies in this within-unit variance, that brings extra information in addition to the traditional between-unit variance observed in cross-sectional data (Capitaine, Genuer & Thiébaut, 2021).\nPanel or longitudinal data is therefore a special case of clustered data, where data points belong to a cluster that causes a correlation between them. A different type of clustered data is hierarchical data where data points are connected not because they belong to the same unit of analysis, but because the unit of analysis belongs to some overarching group. For example, test scores of students have a hierarchical dimension since the unit of analysis, the student, belongs to a group, a class. All grades from students of the same class can be expected to show some correlation due to characteristics of the class, such as the teacher, the class size etc."
  },
  {
    "objectID": "blog/panel_data_note.html#analyzing-panel-data",
    "href": "blog/panel_data_note.html#analyzing-panel-data",
    "title": "A Note on Panel Data Methods",
    "section": "",
    "text": "Both econometrics and statistics have a large array of methods to analyze panel data, but the terminology used to denote them can often be conflicting and confusing (Gelman, 2005). In general, we can define a longitudinal model with individual specific intercepts as follows:\nY_{it} = f(X_{it}) + c_i + e_{it}\nwhere:\n\n\\(f(X_{it})\\) is an unknown function\n\n\\(Y_{it}\\) represents the outcome\n\n\\(e_{it}\\) is a standard normally distributed error\n\n\\(c_i\\) is an unobserved, time-constant factor specific to individual i\n\nIn econometrics, this model is called the unobserved effects model (Wooldridge, 2010). There, the discussion usually centers around whether ci must be treated as a random variable (i.e., a random effect) or a parameter to be estimated (i.e., a fixed effect).\nOver time:\n\n“Random effects” has become synonymous with zero correlation between \\(c_i\\) and \\(X_i\\)\n\n“Fixed effects” implies correlation between \\(c_i\\) and \\(X_i\\)\n\nIn statistics, on the other hand, a fixed effect is a common term used to refer to any parameter in a regression model that is a population average to be estimated, usually denoted by \\(\\beta\\) (StackExchange, 2012b).\nEstimation under the assumption of non-zero correlation in econometrics (“fixed effects estimation”) is more realistic, but requires the removal of the influence of \\(c_i\\). This can be done using the “within” transformation of the equation to be estimated, which is equivalent to estimating with OLS\nY_{it} - Ȳ_i = f(X_{it} - X̄_i) + e_{it} - ē_i\nWhile this approach successfully removes the influence of \\(c_i\\), it also removes all time-constant variables. The main downside of this approach is that other time-constant variables cannot be included. Estimation under the assumption of no correlation (“random effects estimation”), does allow for the inclusion of time-constant variables, but is often less realistic. This involves estimating\nY_{it} = f(X_{it}) + v_{it}\nwhere \\(v_{it} = c_i + e_{it}\\). Econometricians account for the serial correlation introduced in the composite error term by using generalised least squares. Similar approaches exist in statistics to model the influence of individual-specific factors. They all fall under the “generalised linear mixed model” (GLMM). The general linear mixed model can be written as\nY_{it} = f(X_{it}) + W_{it} * c_i + e_{it}\nWhere:\n\n\\(X_{it}\\) are covariates with fixed effects\n\n\\(W_{it}\\) are covariates with random effects\n\nThese last two equations are equivalent if we only include a random intercept (\\(W_{it} = 1\\)). What the econometrician calls unobserved effects, the statistician calls random intercepts (StackExchange, 2012b).\nCaution is needed, however. While the econometrician’s random effects model is equivalent to the statistician’s random intercept model, estimating both (e.g., using the plm and lmer packages in R) can yield different results as mixed models are usually estimated using (restricted or unrestricted) maximum likelihood and not generalised least squares (StackExchange, 2016).\n“The econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on the machine, the expressions for the estimators are usually rather simple. ML estimation of longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence criteria.” (Croissant & Millo, 2008, p.34)\nThe GLMM does add the opportunity to also add random, individual specific, slopes for covariates in addition to their general, fixed effect. This can be done by including a variable in both Xit and Wit (Zablotski, 2019c). These random effects represent random variation in the model that can be attributed to a group to which observations belong (Zablotski, 2019a). It is with these types of models that opportunities arise to model not only longitudinal but also hierarchical effects simultaneously (Zablotski, 2019b).\nIn statistics, the econometrician’s worry about violation of the “random effects” assumption still holds when working with mixed models and observational data (StackExchange, 2012a). There might still be correlation between the error term and the explanatory variables due to other time-constant unobserved effects. Experimental data does not suffer from this type of endogeneity.\nSpecifically for the case of the random intercept, which is the most prevalent in econometrics, we can employ a “correlated random effects approach”, which explicitly models the correlation between \\(X_{it}\\) and \\(c_i\\). This approach assumes\nc_i = \\psi + X̄_i * \\phi + a_i\nwhere:\n\n\\(X̄_i\\) is the vector of time-averages of all variables\n$E(a_i|X_i) = 0\n\nSo that we can estimate:\nY_{it} = f(X_{it}) + \\psi + Z_i * \\delta + X̄_i * \\phi + v_{it}\nEconometricians again use GLS to deal with serial correlation in the composite error \\(v_{it}\\),all while allowing other time-constant factors and accounting for unobserved time-constant effects that are correlated with \\(X_{it}\\) (Wooldridge, 2010)."
  },
  {
    "objectID": "blog/panel_data_note.html#final-note",
    "href": "blog/panel_data_note.html#final-note",
    "title": "A Note on Panel Data Methods",
    "section": "",
    "text": "In addition to the methods described above, econometrics has a different set of methods for analyzing ‘dynamic’ panel data methods, that include lags of variables. I consider these to be outside of the scope of the current discussion."
  }
]